    随着数字化内容的爆炸式增长，文本相似度计算在学术不端检测、内容去重、搜索与推荐等场景中的重要性日益凸显。对于中文任务，字符级 n-gram 方法因实现简单、鲁棒性较强而被广泛采用：先进行统一的文本规范化（移除 UTF-8 BOM、压缩连续空白为单个空格、裁剪首尾空白），再用长度为 n 的滑动窗口切分字符序列，统计各片段的出现频率，从而将文本形式化为稀疏计数向量。两个向量之间的接近程度通常用余弦相似度度量；当文本过短不足以产生 n-gram 时，则使用字符集合的 Jaccard 指标作为退化方案，以保证边界情形下的稳定性与可解释性。n 的选择体现了对灵敏度与噪声的权衡：n 过小会带来片段碰撞、虚高的相似分；n 过大又会放大细微改写导致误判为不相似。综合中文改写习惯与实验观察，n=2 往往是稳妥的默认值。
    在工程实现层面，性能的主要开销来自两个环节：一是 n-gram 的生成与计数，二是向量间点积与范数计算。前者可通过高效的数据结构（如 Counter）与缓存机制降低常数开销：对重复输入命中规范化与切片的 LRU 缓存，可显著减少不必要的重复计算；对计数过程使用底层 C 实现的计数器，可避免 Python 级的字典累加热点。后者则可通过只遍历较短稀疏向量、在循环内绑定局部方法等方式小幅降低属性查找的成本。在大多数通用负载下，这些“实现级”优化不改变算法语义，却能带来可观的加速收益，使系统在更大文本或更高并发下保持可用。
    需要强调的是，字符级相似度只刻画“表层形式”的相似，与法律或伦理意义上的抄袭判定并不等价。一个务实的系统通常采用分层策略：以 n-gram + 余弦的轻量模块作为快速召回，再叠加 TF-IDF、SimHash 或 MinHash-LSH 进行二次筛选，结合自适应阈值、长度正则化以及领域规则来抑制过短文本、模板句与排版噪声的影响。实际部署还需考虑输入清洗、编码统一、异常处理与可观测性：例如对不存在的文件路径显式抛出 FileNotFoundError 以便上层捕获并记录；对输出路径的写入失败提供可诊断的错误信息；对非法参数（如 n≤0）在模块内快速失败。通过这些工程与算法层面的协同设计，可以构建一个既高效、又稳健、同时具有良好可解释性的中文文本相似度计算模块。
